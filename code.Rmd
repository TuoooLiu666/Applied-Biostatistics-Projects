---
title: "code"
author: "Tuo Liu, Ran Wei, Yujung Chen"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
---


### environment setup if needed
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(DataExplorer, dplyr, arsenal, tidyverse, broom, MASS, MKmisc)
```

### Data read-in
### 3 Numeric criteria to get healthy participants
```{r}
# read in data & create PRE variable
data <- read.csv("./data/frmgham.csv") %>% mutate(PRE = ifelse(PREVCHD+PREVAP+PREVMI+PREVSTRK+PREVHYP+DIABETES >= 1, 1, 0))
```


### Data Preparation
```{r}
# filtration: healthy smoker at exam (1-2, 1-3)
period1 <- data %>% filter(PERIOD==1)
period2 <- data %>% filter(PERIOD==2)
period3 <- data %>% filter(PERIOD==3)

# healthy: using other numeric criteria results in o observation
healthy_current_smoker_p1 <- period1 %>% filter(CURSMOKE == 1 & PRE == 0)
healthy_current_smoker_p2 <- period2 %>% filter(RANDID %in% healthy_current_smoker_p1$RANDID) # forget about the name
healthy_current_smoker_p3 <- period3 %>% filter(RANDID %in% healthy_current_smoker_p1$RANDID) # forget about the name

# data
period_1 <- healthy_current_smoker_p1
period_2 <- healthy_current_smoker_p2
period_3 <- healthy_current_smoker_p3


# data type transformation
period_1$SEX <- as.factor(period_1$SEX)
period_1$PERIOD <- as.factor(period_1$PERIOD)
period_1$CURSMOKE <- as.factor(period_1$CURSMOKE)
period_1$CVD <- as.factor(period_1$CVD)

period_2$SEX <- as.factor(period_2$SEX)
period_2$PERIOD <- as.factor(period_2$PERIOD)
period_2$CURSMOKE <- as.factor(period_2$CURSMOKE)
period_2$CVD <- as.factor(period_2$CVD)


period_3$SEX <- as.factor(period_3$SEX)
period_3$PERIOD <- as.factor(period_3$PERIOD)
period_3$CURSMOKE <- as.factor(period_3$CURSMOKE)
period_3$CVD <- as.factor(period_3$CVD)


###### updated period 3 contains current smoker who's been smoking since period 1
###### and smoking quitter who quitted smoking since period 1.
all_period_smoker <- period_3[(period_3$CURSMOKE == "1") & 
                                period_3$RANDID %in% period_2[period_2$CURSMOKE == "1",]$RANDID, ]
all_period_quiter <- period_3[(period_3$CURSMOKE == "0") & 
                                period_3$RANDID %in% period_2[period_2$CURSMOKE == "0",]$RANDID, ]
period_3_updated <- rbind(all_period_quiter,all_period_smoker)
```




### Data Explorative Analysis
```{r}
# plots by smoke status
ggplot(data=period_123, aes(x=CURSMOKE, y=BMI)) + geom_boxplot()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=CURSMOKE, y=SYSBP)) + geom_boxplot()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=CURSMOKE, y=DIABP)) + geom_boxplot()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=CURSMOKE, y=TOTCHOL)) + geom_boxplot()+ facet_wrap(~PERIOD)

# plots by sex
ggplot(data=period_123, aes(x=SEX, y=BMI)) + geom_boxplot()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=SEX, y=SYSBP)) + geom_boxplot()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=SEX, y=DIABP)) + geom_boxplot()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=SEX, y=TOTCHOL)) + geom_boxplot()+ facet_wrap(~PERIOD)

# plots by age
ggplot(data=period_123, aes(x=AGE, y=BMI)) + geom_point()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=AGE, y=SYSBP)) + geom_point()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=AGE, y=DIABP)) + geom_point()+ facet_wrap(~PERIOD)
ggplot(data=period_123, aes(x=AGE, y=TOTCHOL)) + geom_point()+ facet_wrap(~PERIOD)

# 2x2 table by sex and smoking
period_2 %>% group_by(SEX,CURSMOKE) %>% mutate(n=n()) %>% 
        group_by(CURSMOKE) %>% distinct(CURSMOKE,SEX,n)%>%
        mutate(Per=n/sum(n), np=paste0(n," (",round(Per*100,2)," %)")) %>%
        dplyr::select(-n,-Per) %>% spread(CURSMOKE,np)

period_3_updated %>% group_by(SEX,CURSMOKE) %>% mutate(n=n()) %>% 
        group_by(CURSMOKE) %>% distinct(CURSMOKE,SEX,n)%>%
        mutate(Per=n/sum(n), np=paste0(n," (",round(Per*100,2)," %)")) %>%
        dplyr::select(-n,-Per) %>% spread(CURSMOKE,np)
```

```{r results="asis"}
# filtration on original data
period_123 <- rbind(period_1, period_2, period_3)
# variabel type
DataExplorer::plot_missing(period_123)
DataExplorer::plot_qq(period_123[,c("AGE", "BMI", "SYSBP", "DIABP", "TOTCHOL")])

# descriptive statistics for table 1
my_labels <- list(
  CVD = "CVD Status",
  PERIOD = "Examination Cycle",
  CURSMOKE = "Current Smoker",
  CIGPDAY = "Cigarettes per day"
)
# attr(data$SEX,'label')  <- 'Gender'
# attr(data$PERIOD,'label')  <- 'Examination Cycle'
# attr(data$CVD,'label')  <- 'CVD Status'


my_controls <- tableby.control(
  test = T,
  total = F,
  numeric.test = "anova", cat.test = "chisq",
  numeric.stats = c("meansd", "medianq1q3", "Nmiss2"),
  cat.stats = c("countpct", "Nmiss2"),
  stats.labels = list(
    meansd = "Mean (SD)",
    medianq1q3 = "Median (Q1, Q3)",
    Nmiss2 = "Missing"
  )
)

table <- arsenal::tableby(interaction(PERIOD, CVD) ~  SEX + CURSMOKE + CIGPDAY + AGE + BMI + SYSBP + DIABP + TOTCHOL, data = period_123, control = my_controls)

summary(table,
  labelTranslations = my_labels,
  title = "Summary Statistic of Framingham Heart Study Longitudinal Data", 
  pfootnote=TRUE,
  results="asis",
  digits=1
)
```




### Model Build
Logistic Regression
Provides the following unique features:
* Hosmer-Lemeshow test of goodness of fit for the model
* Stepwise analyses
* Contrasts to define model parameterization
* Alternative cut points for classification
* Classification plots
* Model fitted on one set of cases to a held-out set of cases
* Saves predictions, residuals, and influence statistics

#### PERIOD_2
- check on class bias
- resample if bias is found
```{r}
# class bias
table(period_2$CVD)

# resample
# Create Training Data
input_ones <- period_2[which(period_2$CVD == 1), ]  # all 1's
input_zeros <- period_2[which(period_2$CVD == 0), ]  # all 0's
set.seed(100)  # for reproducibility of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData_2 <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData_2 <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's
```

- model build/stepwise analysis/goodness-of-fit
```{r}
trainingData_2 <-  trainingData_2 %>% dplyr::select(AGE, BMI,TOTCHOL,SYSBP,GLUCOSE,CVD,SEX,CURSMOKE) %>% na.omit()
logitMod_2 <- glm(CVD ~ SEX + AGE + BMI + CURSMOKE + TOTCHOL + SYSBP + GLUCOSE, data=trainingData_2, family=binomial)
predicted_2 <- predict(logitMod_2, testData_2, type="response")  # predicted scores


# stepwise: backward variable selection based on AIC
backwards <- step(logitMod_2,trace=0) # # Backwards selection is the default , would suppress step by step output.
formula(backwards) # get selected variables


```
SEX2,AGE,TOTCHOL, and SYSBP are selected from Backwards step-wise selection procedure. Hosmer-Lemeshow test of goodness of fit for the model produces a p-value $<0.05$, indicating poor fit of the model for our data.

#### PERIOD_3
- check on class bias
- resample if bias is found

**** I changed the period_3 to period_3_updated in order to test the new dataset *****
```{r}
# class bias
table(period_3_updated$CVD)

# resample
# Create Training Data
input_ones <- period_3_updated[which(period_3_updated$CVD == 1), ]  # all 1's
input_zeros <- period_3_updated[which(period_3_updated$CVD == 0), ]  # all 0's
set.seed(100)  # for reproducibility of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData_3 <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData_3 <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's
```
- model build/stepwise analysis/goodness-of-fit
```{r}
trainingData_3 <-  trainingData_3 %>% dplyr::select(AGE, BMI,TOTCHOL,SYSBP,GLUCOSE,CVD,SEX,CURSMOKE) %>% na.omit()
logitMod_3 <- glm(CVD ~ SEX + AGE + BMI + CURSMOKE + TOTCHOL + SYSBP + GLUCOSE, data=trainingData_3, family=binomial)
predicted_3 <- predict(logitMod_3, testData_3, type="response")  # predicted scores


# stepwise: backward variable selection based on AIC
backwards <- step(logitMod_3,trace=0) # # Backwards selection is the default , would suppress step by step output.
formula(backwards) # get selected variables
```
SEX2,AGE,GLUCOSE are selected from Backwards step-wise selection procedure. Hosmer-Lemeshow test of goodness of fit for the model produces a p-value $<0.05$, indicating poor fit of the model for our data.


### Model Diagnostics

The logistic regression method assumes that:

* The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.
* There is a linear relationship between the logit of the outcome and each predictor variables. Recall that the logit function is logit(p) = log(p/(1-p)), where p is the probabilities of the outcome.
* There is no influential values (extreme values or outliers) in the continuous predictors.
* There is no high inter-correlations (i.e. multicollinearity) among the predictors.


#### Linearity/influential obs
##### Linearity
PERIOD-2
```{r}
probabilities <- predict(logitMod_2, type = "response")

# Select only numeric predictors
mydata <- trainingData_2 %>% 
  dplyr::select(AGE , BMI, TOTCHOL , SYSBP , GLUCOSE) %>% 
  na.omit()
predictors <- colnames(mydata)

# Bind the logit and tidying the data for plot
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>% 
  gather(key = "predictors", value = "predictor.value", -logit)


ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

PERIOD-3
```{r}
probabilities <- predict(logitMod_3, type = "response")

# Select only numeric predictors
mydata <- trainingData_3 %>% 
  dplyr::select(AGE , BMI, TOTCHOL , SYSBP , GLUCOSE) %>% 
  na.omit()
predictors <- colnames(mydata)

# Bind the logit and tidying the data for plot
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>% 
  gather(key = "predictors", value = "predictor.value", -logit)


ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```
The smoothed scatter plots show that variables AGE, BMI, SYSBP, GLUCOSE and TOTCHOL are all quite linearly associated with the CVD outcome in logit scale.

##### Influential obs
```{r}
# Extract model results
model.data <- broom::augment(logitMod_2) %>% 
  mutate(index = 1:n())

model.data %>% top_n(3, .cooksd)

# plot
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = CVD), alpha = .5) +
  theme_bw()
```

```{r}
# Extract model results
model.data <- broom::augment(logitMod_3) %>% 
  mutate(index = 1:n())

model.data %>% top_n(3, .cooksd)

# plot
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = CVD), alpha = .5) +
  theme_bw()
```
Three observations with highest standardized residuals have lower than 3 std. residual. So no influential observations are found.

#### multicollinearity
```{r}
# VIF: multicollinearity,  VIF well below 4? Yes
car::vif(logitMod_2)
car::vif(logitMod_3)
```


### Model summary, goodness-of-fit, ROC
```{r warning=FALSE}
summary(logitMod_2)
summary(logitMod_3)
# goodness of fit test: The null hypothesis holds that the model fits the data
MKmisc::HLgof.test(fit = fitted(logitMod_2), obs = trainingData_2$CVD) # p-value < 0.05
# goodness of fit test: The null hypothesis holds that the model fits the data
MKmisc::HLgof.test(fit = fitted(logitMod_3), obs = trainingData_3$CVD) # p-value < 0.05

# ROC
InformationValue::plotROC(testData_2$CVD, predicted_2)
InformationValue::plotROC(testData_2$CVD, predicted_3)
```





#### sensitivity analysis
##### categorizing covariates
According to the model assumption assessment results for linearity between logit and continuous variables, linearity is not shown in all pairs. Moreover, scales of continuous variables before centering may impede model fit. 
<!-- # threshold -->
<!-- # AGE      age groups by 65 1/2 -->
<!-- # BMI 30  healthy/overweight 1/2 -->
<!-- # TOTCHOL 160 low/high 1/2 -->
<!-- # SYSBP  130 low/high 1/2 -->
<!-- # GLUCOSE 200 low/high 1/2 -->
```{r}
# PERIOD_2
period_2$agegrp <- ifelse(period_2$AGE>65,2, 1) %>% as.factor()
period_2$bmigrp <- ifelse(period_2$BMI>=30, 2, 1) %>% as.factor()
period_2$TOTCHOLgrp <- ifelse(period_2$TOTCHOL>=160, 2, 1) %>% as.factor()
period_2$SYSBPgrp <- ifelse(period_2$SYSBP>=130, 2,1) %>% as.factor()
period_2$GLUCOSEgrp <- ifelse(period_2$GLUCOSE>=200, 2, 1) %>% as.factor()


# class bias
table(period_2$CVD)
table(period_2$agegrp)


# resample
# Create Training Data
input_ones <- period_2[which(period_2$CVD == 1), ]  # all 1's
input_zeros <- period_2[which(period_2$CVD == 0), ]  # all 0's
set.seed(100)  # for reproducibility of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData_21 <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData_21 <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's


# model fit
trainingData_21 <-  trainingData_21 %>% dplyr::select(agegrp, bmigrp,TOTCHOLgrp,SYSBPgrp,GLUCOSEgrp,CVD,SEX,CURSMOKE) %>% na.omit()
logitMod_21 <- glm(CVD ~ SEX + agegrp +  bmigrp + CURSMOKE + TOTCHOLgrp + SYSBPgrp, data=trainingData_21, family=binomial)
predicted_21 <- predict(logitMod_21, testData_21, type="response")  # predicted scores
summary(logitMod_21)

# reduced model
# logitMod_22 <- glm(CVD ~ CURSMOKE, data=trainingData_21, family=binomial)
# summary(logitMod_22)


# stepwise: backward variable selection based on AIC
backwards <- step(logitMod_21,trace=0) # # Backwards selection is the default , would suppress step by step output.
formula(backwards) # get selected variables
```
```{r} 
period_3_updated$agegrp <- ifelse(period_3_updated$AGE>65,2, 1) %>% as.factor()
period_3_updated$bmigrp <- ifelse(period_3_updated$BMI>=30, 2, 1) %>% as.factor()
period_3_updated$TOTCHOLgrp <- ifelse(period_3_updated$TOTCHOL>=160, 2, 1) %>% as.factor()
period_3_updated$SYSBPgrp <- ifelse(period_3_updated$SYSBP>=130, 2,1) %>% as.factor()
period_3_updated$GLUCOSEgrp <- ifelse(period_3_updated$GLUCOSE>=200, 2, 1) %>% as.factor()


# class bias
table(period_3_updated$CVD)
table(period_3_updated$agegrp)


# resample
# Create Training Data
input_ones <- period_3_updated[which(period_3_updated$CVD == 1), ]  # all 1's
input_zeros <- period_3_updated[which(period_3_updated$CVD == 0), ]  # all 0's
set.seed(100)  # for reproducibility of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData_31 <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData_31 <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's


# model fit
trainingData_31 <-  trainingData_31 %>% dplyr::select(agegrp, bmigrp,TOTCHOLgrp,SYSBPgrp,GLUCOSEgrp,CVD,SEX,CURSMOKE) %>% na.omit()
logitMod_31 <- glm(CVD ~ SEX + agegrp +  bmigrp + CURSMOKE + TOTCHOLgrp + SYSBPgrp, data=trainingData_31, family=binomial)
predicted_31 <- predict(logitMod_31, testData_31, type="response")  # predicted scores
summary(logitMod_31)

# stepwise: backward variable selection based on AIC
backwards <- step(logitMod_31,trace=0) # # Backwards selection is the default , would suppress step by step output.
formula(backwards) # get selected variables
```


#### Interaction/confounding
##### gender masking smoking:cvd?
```{r}
# check distribution
period_3_updated %>% group_by(SEX,CURSMOKE) %>% mutate(n=n()) %>% 
        group_by(CURSMOKE) %>% distinct(CURSMOKE,SEX,n)%>%
        mutate(Per=n/sum(n), np=paste0(n," (",round(Per*100,2)," %)")) %>%
        dplyr::select(-n,-Per) %>% spread(CURSMOKE,np)

# model fit
logitMod_31 <- glm(CVD ~ SEX + agegrp +  bmigrp + CURSMOKE + TOTCHOLgrp + SYSBPgrp + agegrp:CURSMOKE, data=trainingData_31, family=binomial)
summary(logitMod_31)

```




#### Survival Analysis
#### Load packages and data
```{r}
library(survival)
library(ranger)
library(ggplot2)
library(dplyr)
library(ggfortify)

# Generate continuous smoker through out 3 periods, and their covariates at p1
# Generate smoking quitter since p1, and their covariates at p1.
conti_smoker_p1 <- period_1[period_1$RANDID %in% all_period_smoker$RANDID,]
conti_smoker_p1$smoke <- 1
quiter_p1 <- period_1[period_1$RANDID %in% all_period_quiter$RANDID,]
quiter_p1$smoke <- 0
survival_p1 <- rbind(conti_smoker_p1, quiter_p1)
survival_p1$CVD <- as.numeric(survival_p1$CVD)
```

### Categorizing covariates
```{r}
survival_p1$smoke<- ifelse(survival_p1$smoke == 1, "Persistent Smoker", "Smoking Quiter") %>% as.factor()
survival_p1$smoke = relevel(survival_p1$smoke, ref = "Persistent Smoker")
survival_p1$agegrp <- ifelse(survival_p1$AGE>=50,"50 and older", "younger than 50") %>% as.factor()
survival_p1$agegrp = relevel(survival_p1$agegrp, ref = "younger than 50")
survival_p1$bmigrp <- ifelse(survival_p1$BMI>=25, "Overweight", "Normal") %>% as.factor()
survival_p1$bmigrp = relevel(survival_p1$bmigrp, ref = "Normal")
survival_p1$TOTCHOLgrp <- ifelse(survival_p1$TOTCHOL>=200, "High Cholesterol", "Normal") %>% as.factor()
survival_p1$TOTCHOLgrp = relevel(survival_p1$TOTCHOLgrp, ref = "Normal")
survival_p1$SYSBPgrp <- ifelse(survival_p1$SYSBP>=130, "High Blood Pressure","Normal") %>% as.factor()
survival_p1$SYSBPgrp = relevel(survival_p1$SYSBPgrp, ref = "Normal")
survival_p1$GLUCOSEgrp <- ifelse(survival_p1$GLUCOSE>=100, "High Glucose", "Normal") %>% as.factor()
survival_p1$GLUCOSEgrp = relevel(survival_p1$GLUCOSEgrp, ref = "Normal")
survival_p1$CIGPDAYgrp <- ifelse(survival_p1$CIGPDAY <= 15, "Light Smoker", "Heavy Smoker") %>% as.factor()
survival_p1$CIGPDAYgrp = relevel(survival_p1$CIGPDAYgrp, ref = "Light Smoker")
survival_p1$SEXgrp <- ifelse(survival_p1$SEX == 2, "Women", "Men") %>% as.factor()
survival_p1$SEXgrp = relevel(survival_p1$SEXgrp, ref = "Women")
```

#### Survival Model of CVD in relation to quiting smoking
```{r}
kaplan_meier <- with(survival_p1, Surv(TIMECVD, CVD))
km_fit <- survfit(Surv(TIMECVD, CVD) ~ smoke, data = survival_p1)
autoplot(km_fit)

km_fit <- survfit(Surv(TIMECVD, CVD) ~ CIGPDAYgrp, data = survival_p1)
autoplot(km_fit)

km_fit <- survfit(Surv(TIMECVD, CVD) ~ SEXgrp, data = survival_p1)
autoplot(km_fit)

km_fit <- survfit(Surv(TIMECVD, CVD) ~ agegrp, data = survival_p1)
autoplot(km_fit)

km_fit <- survfit(Surv(TIMECVD, CVD) ~ bmigrp, data = survival_p1)
autoplot(km_fit)

km_fit <- survfit(Surv(TIMECVD, CVD) ~ TOTCHOLgrp, data = survival_p1)
autoplot(km_fit)

km_fit <- survfit(Surv(TIMECVD, CVD) ~ SYSBPgrp, data = survival_p1)
autoplot(km_fit)
```

#### Cox Proportional Hazards Model
```{r}
cox <- coxph(Surv(TIMECVD, CVD) ~ smoke + CIGPDAYgrp + SEXgrp + agegrp + bmigrp +
                                           TOTCHOLgrp + SYSBPgrp, data = survival_p1)
summary(cox)

cox_fit <- survfit(cox)
autoplot(cox_fit)
```
#### Checking the assumptions of the Cox Proportional Hazard's model
#### i.e covariates do not vary with time
```{r}
aa_fit <- aareg(Surv(TIMECVD, CVD) ~ smoke + CIGPDAYgrp + SEXgrp + agegrp + bmigrp +
                                           TOTCHOLgrp + SYSBPgrp, data = survival_p1)

aa_fit

autoplot(aa_fit)
```
####  Random Forests Model
```{r}
# Ranger model
survival_p1_complete <- na.omit(survival_p1[,c("TIMECVD","CVD", "smoke","CIGPDAYgrp","SEXgrp",
                                               "agegrp", "bmigrp", "TOTCHOLgrp","SYSBPgrp")])
r_fit <- ranger(Surv(TIMECVD, CVD) ~ smoke + CIGPDAYgrp + SEXgrp + agegrp + bmigrp +
                TOTCHOLgrp + SYSBPgrp, data = survival_p1_complete, mtry = 4,
                importance = "permutation", splitrule = "extratrees", verbose = TRUE)

# Average the survival models
CVD_time <- r_fit$unique.death.times
surv_prob <- data.frame(r_fit$survival)
avg_prob <- sapply(surv_prob, mean)

# Plot the survival models for each subjects
plot(r_fit$unique.death.times, r_fit$survival[1,], type = "l", ylim = c(0,1),
     col = "red", xlab = "Days", ylab = "Free from CVD", main = "Subject CVD Curves")

cols <- colors()
for (n in sample(c(2:dim(survival_p1_complete)[1]), 20)){
  lines(r_fit$unique.death.times, r_fit$survival[n,], type = "l", col = cols[n])
}
lines(CVD_time, avg_prob, lwd = 2)
legend(500, 0.7, legend = c('Average = black'))


# Rank variable importance
vi <- data.frame(sort(round(r_fit$variable.importance, 4), decreasing = TRUE))
names(vi) <- "importance"
head(vi)

# Prediction Error
cat("Prediction Error = 1 - Harrell's c-index = ", r_fit$prediction.error)
```
####























