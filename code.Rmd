---
title: "code"
author: "Tuo Liu, Ran Wei, Yujung Chen"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
---


### environment setup if needed
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(DataExplorer, dplyr, arsenal, tidyverse, broom, MASS, MKmisc)
```

### Data read-in
### 3 Numeric criteria to get healthy participants
```{r}
# read in data & create PRE variable
data <- read.csv("./data/frmgham.csv") %>% mutate(PRE = ifelse(PREVCHD+PREVAP+PREVMI+PREVSTRK+PREVHYP+DIABETES >= 1, 1, 0))
```


### Data Preparation
```{r}
# filtration: healthy smoker at exam (1-2, 1-3)
period1 <- data %>% filter(PERIOD==1)
period2 <- data %>% filter(PERIOD==2)
period3 <- data %>% filter(PERIOD==3)

# healthy: using other numeric criteria results in o observation
healthy_current_smoker_p1 <- period1 %>% filter(CURSMOKE == 1 & PRE == 0)
healthy_current_smoker_p2 <- period2 %>% filter(RANDID %in% healthy_current_smoker_p1$RANDID) # forget about the name
healthy_current_smoker_p3 <- period3 %>% filter(RANDID %in% healthy_current_smoker_p1$RANDID) # forget about the name

# data
period_1 <- healthy_current_smoker_p1
period_2 <- healthy_current_smoker_p2
period_3 <- healthy_current_smoker_p3


# data type transformation
period_1$SEX <- as.factor(period_1$SEX)
period_1$PERIOD <- as.factor(period_1$PERIOD)
period_1$CURSMOKE <- as.factor(period_1$CURSMOKE)
period_1$CVD <- as.factor(period_1$CVD)

period_2$SEX <- as.factor(period_2$SEX)
period_2$PERIOD <- as.factor(period_2$PERIOD)
period_2$CURSMOKE <- as.factor(period_2$CURSMOKE)
period_2$CVD <- as.factor(period_2$CVD)


period_3$SEX <- as.factor(period_3$SEX)
period_3$PERIOD <- as.factor(period_3$PERIOD)
period_3$CURSMOKE <- as.factor(period_3$CURSMOKE)
period_3$CVD <- as.factor(period_3$CVD)


###### updated period 3 contains current smoker who's been smoking since period 1
###### and smoking quitter who quitted smoking since period 1.
all_period_smoker <- period_3[(period_3$CURSMOKE == "1") & 
                                period_3$RANDID %in% period_2[period_2$CURSMOKE == "1",]$RANDID, ]
all_period_quiter <- period_3[(period_3$CURSMOKE == "0") & 
                                period_3$RANDID %in% period_2[period_2$CURSMOKE == "0",]$RANDID, ]
period_3_updated <- rbind(all_period_quiter,all_period_smoker)
```




### Data Explorative Analysis
```{r results="asis"}
# filtration on original data
period_123 <- rbind(period_1, period_2, period_3)
# variabel type
DataExplorer::plot_missing(period_123)
DataExplorer::plot_qq(period_123[,c("AGE", "BMI", "SYSBP", "DIABP", "TOTCHOL")])

# descriptive statistics for table 1
my_labels <- list(
  CVD = "CVD Status",
  PERIOD = "Examination Cycle",
  CURSMOKE = "Current Smoker",
  CIGPDAY = "Cigarettes per day"
)
# attr(data$SEX,'label')  <- 'Gender'
# attr(data$PERIOD,'label')  <- 'Examination Cycle'
# attr(data$CVD,'label')  <- 'CVD Status'


my_controls <- tableby.control(
  test = T,
  total = F,
  numeric.test = "anova", cat.test = "chisq",
  numeric.stats = c("meansd", "medianq1q3", "Nmiss2"),
  cat.stats = c("countpct", "Nmiss2"),
  stats.labels = list(
    meansd = "Mean (SD)",
    medianq1q3 = "Median (Q1, Q3)",
    Nmiss2 = "Missing"
  )
)

table <- arsenal::tableby(interaction(PERIOD, CVD) ~  SEX + CURSMOKE + CIGPDAY + AGE + BMI + SYSBP + DIABP + TOTCHOL, data = period_123, control = my_controls)

summary(table,
  labelTranslations = my_labels,
  title = "Summary Statistic of Framingham Heart Study Longitudinal Data", 
  pfootnote=TRUE,
  results="asis",
  digits=1
)
```



### Model Build
Logistic Regression
Provides the following unique features:
* Hosmer-Lemeshow test of goodness of fit for the model
* Stepwise analyses
* Contrasts to define model parameterization
* Alternative cut points for classification
* Classification plots
* Model fitted on one set of cases to a held-out set of cases
* Saves predictions, residuals, and influence statistics

#### PERIOD_2
- check on class bias
- resample if bias is found
```{r}
# class bias
table(period_2$CVD)

# resample
# Create Training Data
input_ones <- period_2[which(period_2$CVD == 1), ]  # all 1's
input_zeros <- period_2[which(period_2$CVD == 0), ]  # all 0's
set.seed(100)  # for reproducibility of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData_2 <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData_2 <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's
```

- model build/stepwise analysis/goodness-of-fit
```{r}
trainingData_2 <-  trainingData_2 %>% dplyr::select(AGE, BMI,TOTCHOL,SYSBP,GLUCOSE,CVD,SEX,CURSMOKE) %>% na.omit()
logitMod_2 <- glm(CVD ~ SEX + AGE + BMI + CURSMOKE + TOTCHOL + SYSBP + GLUCOSE, data=trainingData_2, family=binomial)
predicted_2 <- predict(logitMod_2, testData_2, type="response")  # predicted scores


# stepwise: backward variable selection based on AIC
backwards <- step(logitMod_2,trace=0) # # Backwards selection is the default , would suppress step by step output.
formula(backwards) # get selected variables


```
SEX2,AGE,TOTCHOL, and SYSBP are selected from Backwards step-wise selection procedure. Hosmer-Lemeshow test of goodness of fit for the model produces a p-value $<0.05$, indicating poor fit of the model for our data.

#### PERIOD_3
- check on class bias
- resample if bias is found

**** I changed the period_3 to period_3_updated in order to test the new dataset *****
```{r}
# class bias
table(period_3_updated$CVD)

# resample
# Create Training Data
input_ones <- period_3_updated[which(period_3_updated$CVD == 1), ]  # all 1's
input_zeros <- period_3_updated[which(period_3_updated$CVD == 0), ]  # all 0's
set.seed(100)  # for reproducibility of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData_3 <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData_3 <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's
```
- model build/stepwise analysis/goodness-of-fit
```{r}
trainingData_3 <-  trainingData_3 %>% dplyr::select(AGE, BMI,TOTCHOL,SYSBP,GLUCOSE,CVD,SEX,CURSMOKE) %>% na.omit()
logitMod_3 <- glm(CVD ~ SEX + AGE + BMI + CURSMOKE + TOTCHOL + SYSBP + GLUCOSE, data=trainingData_3, family=binomial)
predicted_3 <- predict(logitMod_3, testData_3, type="response")  # predicted scores


# stepwise: backward variable selection based on AIC
backwards <- step(logitMod_3,trace=0) # # Backwards selection is the default , would suppress step by step output.
formula(backwards) # get selected variables

```
SEX2,AGE,GLUCOSE are selected from Backwards step-wise selection procedure. Hosmer-Lemeshow test of goodness of fit for the model produces a p-value $<0.05$, indicating poor fit of the model for our data.


### Model Diagnostics

The logistic regression method assumes that:

* The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.
* There is a linear relationship between the logit of the outcome and each predictor variables. Recall that the logit function is logit(p) = log(p/(1-p)), where p is the probabilities of the outcome.
* There is no influential values (extreme values or outliers) in the continuous predictors.
* There is no high inter-correlations (i.e. multicollinearity) among the predictors.


#### Linearity/influential obs
##### Linearity
PERIOD-2
```{r}
probabilities <- predict(logitMod_2, type = "response")

# Select only numeric predictors
mydata <- trainingData_2 %>% 
  dplyr::select(AGE , BMI, TOTCHOL , SYSBP , GLUCOSE) %>% 
  na.omit()
predictors <- colnames(mydata)

# Bind the logit and tidying the data for plot
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>% 
  gather(key = "predictors", value = "predictor.value", -logit)


ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

PERIOD-3
```{r}
probabilities <- predict(logitMod_3, type = "response")

# Select only numeric predictors
mydata <- trainingData_3 %>% 
  dplyr::select(AGE , BMI, TOTCHOL , SYSBP , GLUCOSE) %>% 
  na.omit()
predictors <- colnames(mydata)

# Bind the logit and tidying the data for plot
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>% 
  gather(key = "predictors", value = "predictor.value", -logit)


ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```
The smoothed scatter plots show that variables AGE, BMI, SYSBP, GLUCOSE and TOTCHOL are all quite linearly associated with the CVD outcome in logit scale.

##### Influential obs
```{r}
# Extract model results
model.data <- broom::augment(logitMod_2) %>% 
  mutate(index = 1:n())

model.data %>% top_n(3, .cooksd)

# plot
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = CVD), alpha = .5) +
  theme_bw()
```

```{r}
# Extract model results
model.data <- broom::augment(logitMod_3) %>% 
  mutate(index = 1:n())

model.data %>% top_n(3, .cooksd)

# plot
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = CVD), alpha = .5) +
  theme_bw()
```
Three observations with highest standardized residuals have lower than 3 std. residual. So no influential observations are found.

#### multicollinearity
```{r}
# VIF: multicollinearity,  VIF well below 4? Yes
car::vif(logitMod_2)
car::vif(logitMod_3)
```


### Model summary, goodness-of-fit, ROC
```{r warning=FALSE}
summary(logitMod_2)
summary(logitMod_3)
# goodness of fit test: The null hypothesis holds that the model fits the data
MKmisc::HLgof.test(fit = fitted(logitMod_2), obs = trainingData_2$CVD) # p-value < 0.05
# goodness of fit test: The null hypothesis holds that the model fits the data
MKmisc::HLgof.test(fit = fitted(logitMod_3), obs = trainingData_3$CVD) # p-value < 0.05

# ROC
InformationValue::plotROC(testData_2$CVD, predicted_2)
InformationValue::plotROC(testData_2$CVD, predicted_3)
```





#### Interaction/confounding
```{r}

```



#### sensitivity analysis
```{r}

```

